# Advanced Brain Tumor Detection with EfficientNet + Attention

This project implements an advanced deep learning architecture for brain tumor detection using EfficientNet-B3 backbone with spatial attention mechanism. It represents a significant upgrade from the original VGG16-based model.

## ğŸš€ Key Improvements

### Architecture Upgrade
- **BEFORE**: VGG16 (138M parameters, 2014 architecture)
- **AFTER**: EfficientNet-B3 (12M parameters, modern architecture)
- **Improvement**: 91% parameter reduction with better accuracy

### Enhanced Features
- âœ… **Spatial Attention Mechanism**: Focuses on tumor regions for better detection
- âœ… **Mixed Precision Training**: 2x faster training with maintained accuracy
- âœ… **Advanced Data Augmentation**: Comprehensive augmentation pipeline
- âœ… **Cosine Decay LR Scheduling**: Optimal learning rate scheduling with warmup
- âœ… **AdamW Optimizer**: Better optimization with weight decay
- âœ… **Comprehensive Callbacks**: Early stopping, model checkpointing, TensorBoard logging

### Performance Targets
- **Accuracy**: 97-99% (vs original ~95%)
- **Training Speed**: 2-3x faster
- **Memory Usage**: Significantly reduced
- **Generalization**: Better performance on unseen data

## ğŸ“ Project Structure

```
â”œâ”€â”€ brain_tumor_efficientnet_attention.ipynb    # Main notebook with complete implementation
â”œâ”€â”€ efficientnet_attention_model.py             # EfficientNet + Attention model architecture
â”œâ”€â”€ data_utils.py                               # Advanced data processing and augmentation
â”œâ”€â”€ training_utils.py                           # Training utilities with callbacks and scheduling
â”œâ”€â”€ evaluation_utils.py                         # Comprehensive evaluation and visualization
â”œâ”€â”€ main_efficientnet_brain_tumor.py           # Command-line script for training/evaluation
â”œâ”€â”€ requirements.txt                            # Python dependencies
â”œâ”€â”€ brain_tumour_detection_using_deep_learning.ipynb  # Original VGG16 implementation
â””â”€â”€ README.md                                   # This file
```

## ğŸ› ï¸ Installation

1. Clone the repository:
```bash
git clone https://github.com/Benguerine/ml-notes-and-projects.git
cd ml-notes-and-projects
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

## ğŸ“Š Usage

### Option 1: Jupyter Notebook (Recommended)
Run the comprehensive notebook:
```bash
jupyter notebook brain_tumor_efficientnet_attention.ipynb
```

### Option 2: Command Line Script
```bash
python main_efficientnet_brain_tumor.py \
    --train_dir "/path/to/training/data" \
    --test_dir "/path/to/test/data" \
    --epochs 30 \
    --batch_size 32 \
    --learning_rate 0.001
```

### Option 3: Individual Module Usage
```python
from efficientnet_attention_model import create_brain_tumor_model
from data_utils import create_data_generators
from training_utils import create_training_manager

# Create model
model, model_builder = create_brain_tumor_model(num_classes=4)

# Setup data
data_generator = create_data_generators(train_dir, test_dir)

# Train model
trainer = create_training_manager(model)
history = trainer.train_model(train_dataset, test_dataset)
```

## ğŸ—ï¸ Architecture Details

### EfficientNet-B3 + Spatial Attention
```
Input (224x224x3)
    â†“
EfficientNet-B3 Backbone (Pre-trained)
    â†“
Spatial Attention Module
    â†“
Global Average Pooling + BatchNorm + Dropout
    â†“
Dense(512) + BatchNorm + Dropout
    â†“
Dense(256) + BatchNorm + Dropout
    â†“
Dense(4, softmax) - Output
```

### Spatial Attention Mechanism
- Computes channel-wise average and max pooling
- Generates spatial attention weights via convolution
- Applies attention to highlight tumor regions
- Improves model interpretability and accuracy

## ğŸ“ˆ Training Features

### Advanced Data Augmentation
- Random brightness/contrast adjustment
- Horizontal and vertical flips
- Random rotation and translation
- Random zoom and scaling
- ImageNet normalization

### Training Optimizations
- **Mixed Precision**: FP16 training for speed
- **Cosine Decay**: Learning rate scheduling with warmup
- **AdamW Optimizer**: Weight decay regularization
- **Class Weights**: Handle imbalanced datasets
- **Early Stopping**: Prevent overfitting
- **Model Checkpointing**: Save best model

### Monitoring and Callbacks
- TensorBoard integration
- CSV training logs
- Learning rate logging
- Automatic learning rate reduction
- Comprehensive training visualizations

## ğŸ“Š Evaluation Features

### Comprehensive Metrics
- Confusion matrices (counts and percentages)
- ROC curves for all classes
- Precision-Recall curves
- Prediction confidence distributions
- Per-class performance metrics

### Attention Visualization
- Spatial attention heatmaps
- Overlay visualizations
- Focus region analysis
- Model interpretability insights

## ğŸ¯ Model Classes

The model classifies brain tumors into 4 categories:
1. **Glioma**: Malignant tumor in glial cells
2. **Meningioma**: Tumor in meninges
3. **Pituitary**: Tumor in pituitary gland
4. **No Tumor**: Healthy brain tissue

## ğŸ“Š Performance Comparison

| Metric | VGG16 (Original) | EfficientNet + Attention |
|--------|------------------|--------------------------|
| Parameters | 138M | 12M (-91%) |
| Training Speed | Baseline | 2-3x faster |
| Memory Usage | High | Significantly reduced |
| Architecture | 2014 | Modern (2019) |
| Attention | None | Spatial attention |
| Target Accuracy | ~95% | 97-99% |

## ğŸ”§ Customization

### Model Architecture
```python
# Adjust model parameters
model_builder = EfficientNetAttentionModel(
    num_classes=4,
    input_shape=(224, 224, 3),
    dropout_rate=0.3  # Adjust dropout
)
```

### Data Augmentation
```python
# Adjust augmentation strength
data_generator = create_data_generators(
    augmentation_strength='light'  # 'light', 'medium', 'strong'
)
```

### Training Configuration
```python
# Customize training parameters
trainer.train_model(
    epochs=30,
    initial_lr=0.001,
    warmup_epochs=2
)
```

## ğŸ“ Key Files Description

- **`efficientnet_attention_model.py`**: Core model architecture with EfficientNet-B3 backbone and spatial attention mechanism
- **`data_utils.py`**: Advanced data loading, preprocessing, and augmentation utilities
- **`training_utils.py`**: Training manager with cosine decay scheduling, callbacks, and mixed precision support
- **`evaluation_utils.py`**: Comprehensive evaluation metrics, visualizations, and attention analysis
- **`main_efficientnet_brain_tumor.py`**: Command-line interface for training and evaluation
- **`brain_tumor_efficientnet_attention.ipynb`**: Complete notebook implementation with visualizations

## ğŸ–ï¸ Benefits of the New Architecture

1. **Efficiency**: 91% fewer parameters while maintaining/improving accuracy
2. **Speed**: 2-3x faster training with mixed precision
3. **Interpretability**: Spatial attention shows model focus areas
4. **Generalization**: Advanced augmentation improves robustness
5. **Modern Architecture**: EfficientNet-B3 vs outdated VGG16
6. **Production Ready**: Comprehensive evaluation and monitoring

## ğŸ”„ Migration from VGG16

The new implementation maintains compatibility with existing data formats while providing significant improvements:

- Same input/output format
- Compatible with existing datasets
- Enhanced preprocessing pipeline
- Better performance metrics
- Comprehensive evaluation tools

## ğŸ“š References

- EfficientNet: [Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946)
- Attention Mechanisms: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- Mixed Precision Training: [TensorFlow Mixed Precision](https://www.tensorflow.org/guide/mixed_precision)

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit issues, feature requests, or pull requests.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details.